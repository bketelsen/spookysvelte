<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta http-equiv="content-security-policy" content=""><title>Organizing Documents with Some AI, ML, and Elbow Grease | Brian Ketelsen</title><meta name="description" content="In this first post of (likely) a multi-part series I&#39;m going to discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my `~/Documents/Unfiled` directory." data-svelte="svelte-vst9xl">
	<link rel="stylesheet" href="/_app/assets/pages/__layout.svelte-24b966e1.css">
	<link rel="modulepreload" href="/_app/start-c742949e.js">
	<link rel="modulepreload" href="/_app/chunks/index-903c2de0.js">
	<link rel="modulepreload" href="/_app/pages/__layout.svelte-b208d1d7.js">
	<link rel="modulepreload" href="/_app/chunks/stores-4a3ff360.js">
	<link rel="modulepreload" href="/_app/pages/posts/_slug_.svelte-cb77a9a7.js">
	<link rel="modulepreload" href="/_app/chunks/PageTitle-e5125eb9.js">
	<link rel="modulepreload" href="/_app/chunks/PrimaryTag-6b8548c8.js">
	</head>
	<body>
		<div>




<aside class="absolute z-40 w-full h-full transition duration-500 ease-in-out transform bg-neutral-focus -left-full"><div class="flex flex-col text-xl text-neutral-content"><a href="/" class="btn btn-ghost btn-sm rounded-btn">Home</a><a href="/pages/about/" class="btn btn-ghost btn-sm rounded-btn">About</a><a href="/pages/uses/" class="btn btn-ghost btn-sm rounded-btn">Uses</a><a href="/pages/now/" class="btn btn-ghost btn-sm rounded-btn">Now</a></div></aside>
<div class="flex h-screen flex-col justify-between"><nav class="navbar bg-neutral text-neutral-content"><div class="flex-none"><button class="z-50 btn btn-square btn-ghost lg:hidden"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" class="inline-block w-10 h-10 stroke-current"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button></div>
    <div class="items-center flex-1"><div class="px-2 pb-2 mx-2 flex-0"><a class="text-3xl font-bold text-primary" href="/"><img class="w-30 h-10" src="/_app/assets/brian-dark-55ba719d.svg" alt="brian ketelsen"></a></div>
      <div class="flex-1 hidden px-2 mx-2 lg:block"><div class="items-stretch"><a href="/" class="btn btn-ghost btn-sm rounded-btn">Home</a><a href="/pages/about/" class="btn btn-ghost btn-sm rounded-btn">About</a><a href="/pages/uses/" class="btn btn-ghost btn-sm rounded-btn">Uses</a><a href="/pages/now/" class="btn btn-ghost btn-sm rounded-btn">Now</a></div></div></div>
    <div class="flex-none"><div class="z-30 dropdown dropdown-left"><div tabindex="0" class="btn btn-ghost"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" class="inline-block w-6 h-6 stroke-current md:mr-2"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21a4 4 0 01-4-4V5a2 2 0 012-2h4a2 2 0 012 2v12a4 4 0 01-4 4zm0 0h12a2 2 0 002-2v-4a2 2 0 00-2-2h-2.343M11 7.343l1.657-1.657a2 2 0 012.828 0l2.829 2.829a2 2 0 010 2.828l-8.486 8.485M7 17h.01"></path></svg>
    <span class="hidden md:block">Theme</span></div>
  <ul tabindex="0" class="pb-2 shadow menu dropdown-content bg-neutral-focus w-52"><li><button class="mx-4 mt-2 btn btn-primary" data-set-theme="cyberpunk" data-act-class="ACTIVECLASS">cyberpunk</button>
      </li><li><button class="mx-4 mt-2 btn btn-primary" data-set-theme="dark" data-act-class="ACTIVECLASS">dark</button>
      </li><li><button class="mx-4 mt-2 btn btn-primary" data-set-theme="dracula" data-act-class="ACTIVECLASS">dracula</button>
      </li><li><button class="mx-4 mt-2 btn btn-primary" data-set-theme="light" data-act-class="ACTIVECLASS">light</button>
      </li></ul></div></div></nav>
  <main class="mb-auto">
<div class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><article><div class="xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700"><header class="pt-6 xl:pb-6"><div class="space-y-1 text-center"><dl class="space-y-10"><div><dt class="sr-only">Published on</dt>
              <dd class="text-base font-medium leading-6 text-primary"><time datetime="2019-09-03T16:20:00.000-04:00">Tuesday, September 3, 2019</time></dd></div></dl>
          <div><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-base-content sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">Organizing Documents with Some AI, ML, and Elbow Grease</h1></div></div></header>
      <div class="divide-y divide-base-200 pb-8 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0" style="grid-template-rows: auto 1fr;"><dl class="pt-6 pb-10 xl:border-b xl:border-base-200 xl:pt-11 "><dt class="sr-only">Authors</dt><dd><ul class="flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2" key="Brian Ketelsen"><img src="https://brian.dev/content/images/2022/05/logo-copy-1.png" width="38px" height="38px" alt="avatar" class="h-10 w-10 rounded-full">
                <dl class="whitespace-nowrap text-sm font-medium leading-5"><dt class="sr-only">Name</dt><dd class="text-base-content ">Brian Ketelsen
                  </dd><dt class="sr-only">Twitter</dt><dd><a href="https://twitter.com/@bketelsen" class="text-accent hover:text-primary">@bketelsen</a></dd></dl></li></ul></dd></dl>
        <div class="divide-y divide-base-200 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="prose prose-lg max-w-none pt-10 pb-8"><img class="rounded-md shadow-xl" src="https://brian.dev/content/images/2022/05/IGa3Md8wP6g.jpg" alt="Organizing Documents with Some AI, ML, and Elbow Grease">
            <!-- HTML_TAG_START --><!--kg-card-begin: markdown--><p>In this first post of (likely) a multi-part series I'm going to discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my <code>~/Documents/Unfiled</code> directory.</p>
<h3 id="the-problem-statement">The Problem Statement</h3>
<p>There are several contributing factors to the problem. Let's start with the obvious ones:</p>
<ul>
<li>I'm a digital packrat</li>
<li>I'm a single parent of 3 (and therefore busy)</li>
<li>I can be lazy sometimes</li>
<li>I have ADHD, and get easily sidetracked from things I intended to do</li>
</ul>
<p>When my dad passed away last year, it got even worse. Suddenly I was getting all of his mail, bills, correspondence, too. I didn't want to lose it; but I sure wasn't ready to read it all. So I scanned it and dropped it in the <code>Unfiled</code> folder.</p>
<p>So now we're here. Where <code>here</code> is a place where I can't find anything I need and my <code>Documents</code> directory is the definition of <code>hot-mess</code>.</p>
<h3 id="the-goal">The Goal</h3>
<p>I'd like to take that folder of 3000 random unclassified documents and sort them into something more clear. I think sorting them by originating source (Supplier, Vendor, Biller, Organization) is a good first step. Eventually I'd like to sort them by date group too. Probably by Year, then Month.</p>
<p>For a bonus, I'd love to do a <a href="https://docs.microsoft.com/en-us/windows/win32/projfs/projected-file-system?WT.mc_id=none-twitter-brketels">projected filesystem</a> sort of thing in Windows and a <a href="https://9p.io/wiki/plan9/Installing_a_Plan_9_File_Server/index.html">Plan9</a> type server on Mac/Linux using FUSE. It'd be really convenient to be able to get at documents from a Filesystem interface by using different facets like keywords, dates, categories, etc. That might fit more cleanly with the way I think, too. But, again, that's a stretch goal, because we'll need all that metadata first.</p>
<p>If you're old enough to remember <a href="https://arstechnica.com/information-technology/2018/07/the-beos-filesystem/">BeOS Filesystem</a>, it would have solved nearly all of this. Someday we'll get back to the database/filesystem mashup that truly needs to exist.</p>
<h3 id="the-solutions">The Solution(s)</h3>
<p>First, there isn't really a one-step solution to this. It's going to take some work, and I can likely automate MOST of that. But there will still be a good portion of things I can't sort automatically.</p>
<h4 id="step-one">Step One</h4>
<p>As a first step, I wrote a small Go program that calls <a href="https://cda.ms/126">Azure Cognitive Services</a> Vision API to do Optical Character Recognition on all the files that are compatible (PDF and image files). Nearly everything I have is in pdf format, but there are a few TIFF files in there too. This program is in flux right now, so I'm not going to release it as Open Source until it's settled a bit. If I forget - ping me on twitter @bketelsen or email mail@bjk.fyi - and remind me! Related: the code samples in this post are probably garbage, and won't likely match the end result that I publish. I'm sure I'm swallowing errors, and haven't done the slightest bit of refactor/cleanup on this code yet.</p>
<blockquote>
<p>WARNING: <em>Don't cut/paste this code yet, please.</em></p>
</blockquote>
<p>I created a domain type appropriately called <code>Document</code> that stores metadata about files on disk:</p>
<pre><code class="language-go">type Document struct {
	Hash         string
	Path         string
	PreviousPath string
	Operation    *CognitiveOperation
	Results      *CognitiveReadResponse
}
</code></pre>
<p>I'll discuss the fields as they come up, but <code>Path</code> and <code>PreviousPath</code> should be obvious. Current and previous location on disk, so that I can account for file moves with at least a little bit of history.</p>
<p>The pricing for the OCR is really attractive - as of September, 2019 it is:</p>
<blockquote>
<p>0-1M transactions — $1.50 per 1,000 transactions</p>
</blockquote>
<p>I know that I'll be fine tuning the processes that run, and likely running them repeatedly. I wanted to find a way to store the results from the OCR for each document, but I am also aware that I can't use the document name and path as the canonical key to find the document later, because the goal of this app is to move them and rename them appropriately! So I decided to use a hash of the file contents as a key. <code>SHA256</code> seems to be the right algorithm for file contents, low cost computation, low collision chance. So I created a hash function that calculates the <code>SHA256</code> hash of the document after it is read:</p>
<pre><code class="language-go">func (d *Document) GetHash() {

	f, err := os.Open(d.Path)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()

	h := sha256.New()
	if _, err := io.Copy(h, f); err != nil {
		log.Fatal(err)
	}
	d.Hash = fmt.Sprintf(&quot;%x&quot;, h.Sum(nil))
}
</code></pre>
<p>After getting the results of the OCR operation, I set them in the <code>Document</code> type, then persist the metadata to disk in a hidden directory. Currently that's <code>~/.classifier/</code> but, as with all of this, it might change in the future.</p>
<p>The file is stored using the <code>SHA256</code> hash of the contents as the file name, and the <code>Document</code> type is serialized to disk using Go's efficient and lightweight <code>encoding/gob</code> format. While I'm debugging and playing with this code, I decided to also persist the data in <code>json</code> format so it's easier to read. Here's the method on <code>Document</code> that saves/serializes to disk:</p>
<pre><code class="language-go">func (d *Document) SaveMetadata() error {
  fmt.Println(d.Hash)
  //TODO use new XDG config dir location
  // https://tip.golang.org/pkg/os/#UserConfigDir
	filePath := &quot;/home/bjk/.classifier/&quot; + d.Hash // TODO FILEPATH.JOIN
	fmt.Println(filePath)
	file, err := os.OpenFile(filePath, os.O_TRUNC|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		return err
	}
	defer file.Close()
	enc := gob.NewEncoder(file)
	err = enc.Encode(d)
	if err != nil {
		return err
	}
	jfilePath := &quot;/home/bjk/.classifier/&quot; + d.Hash + &quot;.json&quot; // TODO FILEPATH.JOIN
	fmt.Println(jfilePath)
	jfile, err := os.OpenFile(jfilePath, os.O_TRUNC|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		return err
	}
	defer jfile.Close()
	jenc := json.NewEncoder(jfile)
	return jenc.Encode(d)
}
</code></pre>
<p>Lots of bad things happening in there, see above caveats about copying/pasting this code. The important part is the encoding in <code>gob</code> format of the contents of the <code>Document</code> metadata, which is then saved to disk using the <code>SHA256</code> hash as the filename. This is a nice future-proof solution, and provides several benefits.</p>
<ul>
<li>If there is already a file with the same name, it's been processed once.</li>
<li>If the <code>.Path</code> is different from the document I'm inspecting, I might have an exact duplicate, which is a candidate for (soft) deleting</li>
<li>It doesn't matter where the files get moved, as long as the <code>SHA256</code> hash matches, I've got the metadata saved already.</li>
</ul>
<p>This is a very low-tech metadata database, of sorts. It's definitely not optimized for real-time use, but instead for batch operations.</p>
<p>Keeping all the metadata in this format means I can write any number of other tools to read and modify the metadata without worrying too much.</p>
<h3 id="step-two">Step Two</h3>
<p>At this point, I have a directory full of unprocessed files and a way to process them once and save the results so I don't have to re-process them later. It's time to fire off the processing app. I used <a href="https://github.com/spf13/cobra">cobra</a> to build the command-line utility, so I made the root/naked command do the actual calls to Azure Cognitive Services:</p>
<pre><code class="language-bash">go build
./classifier
</code></pre>
<p>This iterates over every file in the <code>~/Documents/Unfiled</code> directory, calling Cognitive Services OCR for the file types that are supported. There is no current mechanism to retrieve metadata from other document types (Word documents, text files, etc). That's a future addition.</p>
<p>After receiving the results, the responses are serialized using the above mentioned <code>gob</code> serialization into <code>~/.classifier/HASH</code></p>
<h3 id="classification">Classification</h3>
<p>Based on the results there are some simple <code>bag of words</code> matches that can be done. Some of the documents I have contain very unique text that is indicative of a particular document type. For example, Bank of America always includes my account number and their address in <code>Wilmington</code>. No other document in my corpus has those two distinct things together, so I can write a simple classifier for all Bank of America documents. I decided to use simple TOML for a configuration file here:</p>
<pre><code>[[entity]]
name = &quot;Bank of America&quot;
directory = &quot;BOA&quot;
keywords = [&quot;Bank of America&quot;,&quot;12345677889&quot;,&quot;Wilmington&quot;]
</code></pre>
<p>Here, I added a sub-command in <code>cobra</code> so I can classify files without re-posting them to Cognitive Services. So I added the <code>classifier process</code> command:</p>
<pre><code class="language-bash">./classifier process
</code></pre>
<p>It currently goes through all the files in <code>Unfiled</code> and checks their metadata for matches against the TOML file. This worked perfectly for several of my external correspondents. It took all the documents from <code>Unfiled</code> and placed them in <code>Filed/{directory}</code>.</p>
<h3 id="what-about-the-rest">What About The Rest?</h3>
<p>There are many documents that aren't easily processed this way though. My next inspiration came in the shower (of course). If you squint enough, or are far enough away, all documents from the same entity of the same type look the same. So all my mortgage statements look the same, but the numbers are different.</p>
<p>I installed ImageMagick, and wrote a script to make a low-resolution thumbnail of each PDF. I made the resolution low enough that the text isn't readable even if you magnify the image.</p>
<p>Then I searched for ways to compare images and came across <a href="https://github.com/rivo/duplo">duplo</a>, which appears to do what I need. It does a hash of the image and allows you to compare other documents to that hash to find a similarity score. Using this type of process my next goal is to group similar documents together by searching for ones with matching or close-to-matching image hashes.</p>
<p>But that'll be probably next weekend. It's been really fun doing this much, and I'm looking forward to seeing how much more I can learn as I go!</p>
<p>Intermediate results:</p>
<p>Before:</p>
<pre><code class="language-bash">2846 Files
</code></pre>
<p>After:</p>
<pre><code class="language-bash">Unfiled\
  2710 Files
Filed\
  136 Files in 2 Directories
</code></pre>
<!--kg-card-end: markdown--><!-- HTML_TAG_END --></div></div>
        <footer><div class="divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-1 xl:gap-x-1 xl:divide-y-0"><div class="py-4 xl:py-8"><h2 class="mb-2 text-xs uppercase tracking-wide text-base-content">Tags
              </h2>
              <div class="flex flex-wrap"><div class="ml-2 badge badge-primary">AI/ML</div></div></div>
            </div>
          <div class="pt-4 xl:pt-8"><a href="/posts" class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400">← Back to the blog
            </a></div></footer></div></div></article></div></main>
  <footer class="footer footer-center p-10 bg-base-200 text-base-content rounded"><div class="grid grid-flow-col gap-4"><a href="/pages/about/" class="btn btn-ghost btn-sm rounded-btn">About Brian</a><a href="/pages/now/" class="btn btn-ghost btn-sm rounded-btn">Now</a><a href="/pages/uses/" class="btn btn-ghost btn-sm rounded-btn">Uses</a></div>
    <div><div class="grid grid-flow-col gap-4"><a href="https://twitter.com/bketelsen"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="fill-current"><path d="M24 4.557c-.883.392-1.832.656-2.828.775 1.017-.609 1.798-1.574 2.165-2.724-.951.564-2.005.974-3.127 1.195-.897-.957-2.178-1.555-3.594-1.555-3.179 0-5.515 2.966-4.797 6.045-4.091-.205-7.719-2.165-10.148-5.144-1.29 2.213-.669 5.108 1.523 6.574-.806-.026-1.566-.247-2.229-.616-.054 2.281 1.581 4.415 3.949 4.89-.693.188-1.452.232-2.224.084.626 1.956 2.444 3.379 4.6 3.419-2.07 1.623-4.678 2.348-7.29 2.04 2.179 1.397 4.768 2.212 7.548 2.212 9.142 0 14.307-7.721 13.995-14.646.962-.695 1.797-1.562 2.457-2.549z"></path></svg></a>
        <a href="https://youtube.com/bketelsen"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="fill-current"><path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"></path></svg></a>
        <a href="https://facebook.com/bketelsen"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="fill-current"><path d="M9 8h-3v4h3v12h5v-12h3.642l.358-4h-4v-1.667c0-.955.192-1.333 1.115-1.333h2.885v-5h-3.808c-3.596 0-5.192 1.583-5.192 4.615v3.385z"></path></svg></a></div></div>
    <div><p>Copyright © 2022 - All right reserved by Brian Ketelsen</p></div></footer></div>


		<script type="module" data-sveltekit-hydrate="j0xeh0">
		import { start } from "/_app/start-c742949e.js";
		start({
			target: document.querySelector('[data-sveltekit-hydrate="j0xeh0"]').parentNode,
			paths: {"base":"","assets":""},
			session: {},
			route: true,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/_app/pages/__layout.svelte-b208d1d7.js"),
						import("/_app/pages/posts/_slug_.svelte-cb77a9a7.js")
				],
				params: {slug:"organizing-documents-with-some-ai-ml-and-elbow-grease"},
				routeId: "posts/[slug]"
			}
		});
	</script><script type="application/json" sveltekit:data-type="data" sveltekit:data-url="/settings">{"status":200,"statusText":"","headers":{"content-type":"application/json; charset=utf-8"},"body":"{\"settings\":{\"title\":\"Brian Ketelsen\",\"description\":\"Developer advocate, keynote speaker, author, open source practitioner.\",\"logo\":\"https://brian.dev/content/images/2022/05/brian-light.svg\",\"icon\":\"https://brian.dev/content/images/2022/05/android-chrome-192x192.png\",\"accent_color\":\"#FF1A75\",\"cover_image\":\"https://brian.dev/content/images/2022/05/logo-copy.png\",\"facebook\":\"bketelsen\",\"twitter\":\"@bketelsen\",\"lang\":\"en\",\"locale\":\"en\",\"timezone\":\"America/New_York\",\"codeinjection_head\":\"\u003Cscript>\\n\\tvar apiKey = \\\"e154a4f547e495edf218e2041a\\\";\\n\\tvar tweetBtn = true;\\n\u003C/script>\",\"codeinjection_foot\":\"\u003Cscript src=\\\"/assets/js/plugin/prism.js\\\">\u003C/script>\\n\u003Cscript defer type=\\\"text/javascript\\\" src=\\\"https://api.pirsch.io/pirsch.js\\\"\\nid=\\\"pirschjs\\\"\\ndata-code=\\\"vWhcy7gh61Rp6p292VnP1SdSSBiwvkij\\\">\u003C/script>\",\"navigation\":[{\"label\":\"Home\",\"url\":\"/\"},{\"label\":\"About\",\"url\":\"/about/\"},{\"label\":\"Uses\",\"url\":\"/uses/\"},{\"label\":\"Now\",\"url\":\"/now/\"}],\"secondary_navigation\":[{\"label\":\"About Brian\",\"url\":\"/about/\"},{\"label\":\"Now\",\"url\":\"/now/\"},{\"label\":\"Uses\",\"url\":\"/uses/\"}],\"meta_title\":\"brian.dev | Brian Ketelsen\",\"meta_description\":\"Brian Ketelsen is a developer advocate, keynote speaker, author, and open source practitioner.\",\"og_image\":null,\"og_title\":\"brian.dev | Brian Ketelsen\",\"og_description\":\"Brian Ketelsen is a developer advocate, keynote speaker, author, and open source practitioner.\",\"twitter_image\":\"https://brian.dev/content/images/2022/05/briandev.png\",\"twitter_title\":\"brian.dev | Brian Ketelsen\",\"twitter_description\":\"Brian Ketelsen is a developer advocate, keynote speaker, author, and open source practitioner.\",\"members_support_address\":\"noreply\",\"url\":\"https://brian.dev/\"}}"}</script><script type="application/json" sveltekit:data-type="props">{"post":{"slug":"organizing-documents-with-some-ai-ml-and-elbow-grease","id":"62889299a640e2020be86e54","uuid":"88d47038-8508-483e-a9fa-9d720c4f0692","title":"Organizing Documents with Some AI, ML, and Elbow Grease","html":"\u003C!--kg-card-begin: markdown-->\u003Cp>In this first post of (likely) a multi-part series I'm going to discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my \u003Ccode>~/Documents/Unfiled\u003C/code> directory.\u003C/p>\n\u003Ch3 id=\"the-problem-statement\">The Problem Statement\u003C/h3>\n\u003Cp>There are several contributing factors to the problem. Let's start with the obvious ones:\u003C/p>\n\u003Cul>\n\u003Cli>I'm a digital packrat\u003C/li>\n\u003Cli>I'm a single parent of 3 (and therefore busy)\u003C/li>\n\u003Cli>I can be lazy sometimes\u003C/li>\n\u003Cli>I have ADHD, and get easily sidetracked from things I intended to do\u003C/li>\n\u003C/ul>\n\u003Cp>When my dad passed away last year, it got even worse. Suddenly I was getting all of his mail, bills, correspondence, too. I didn't want to lose it; but I sure wasn't ready to read it all. So I scanned it and dropped it in the \u003Ccode>Unfiled\u003C/code> folder.\u003C/p>\n\u003Cp>So now we're here. Where \u003Ccode>here\u003C/code> is a place where I can't find anything I need and my \u003Ccode>Documents\u003C/code> directory is the definition of \u003Ccode>hot-mess\u003C/code>.\u003C/p>\n\u003Ch3 id=\"the-goal\">The Goal\u003C/h3>\n\u003Cp>I'd like to take that folder of 3000 random unclassified documents and sort them into something more clear. I think sorting them by originating source (Supplier, Vendor, Biller, Organization) is a good first step. Eventually I'd like to sort them by date group too. Probably by Year, then Month.\u003C/p>\n\u003Cp>For a bonus, I'd love to do a \u003Ca href=\"https://docs.microsoft.com/en-us/windows/win32/projfs/projected-file-system?WT.mc_id=none-twitter-brketels\">projected filesystem\u003C/a> sort of thing in Windows and a \u003Ca href=\"https://9p.io/wiki/plan9/Installing_a_Plan_9_File_Server/index.html\">Plan9\u003C/a> type server on Mac/Linux using FUSE. It'd be really convenient to be able to get at documents from a Filesystem interface by using different facets like keywords, dates, categories, etc. That might fit more cleanly with the way I think, too. But, again, that's a stretch goal, because we'll need all that metadata first.\u003C/p>\n\u003Cp>If you're old enough to remember \u003Ca href=\"https://arstechnica.com/information-technology/2018/07/the-beos-filesystem/\">BeOS Filesystem\u003C/a>, it would have solved nearly all of this. Someday we'll get back to the database/filesystem mashup that truly needs to exist.\u003C/p>\n\u003Ch3 id=\"the-solutions\">The Solution(s)\u003C/h3>\n\u003Cp>First, there isn't really a one-step solution to this. It's going to take some work, and I can likely automate MOST of that. But there will still be a good portion of things I can't sort automatically.\u003C/p>\n\u003Ch4 id=\"step-one\">Step One\u003C/h4>\n\u003Cp>As a first step, I wrote a small Go program that calls \u003Ca href=\"https://cda.ms/126\">Azure Cognitive Services\u003C/a> Vision API to do Optical Character Recognition on all the files that are compatible (PDF and image files). Nearly everything I have is in pdf format, but there are a few TIFF files in there too. This program is in flux right now, so I'm not going to release it as Open Source until it's settled a bit. If I forget - ping me on twitter @bketelsen or email mail@bjk.fyi - and remind me! Related: the code samples in this post are probably garbage, and won't likely match the end result that I publish. I'm sure I'm swallowing errors, and haven't done the slightest bit of refactor/cleanup on this code yet.\u003C/p>\n\u003Cblockquote>\n\u003Cp>WARNING: \u003Cem>Don't cut/paste this code yet, please.\u003C/em>\u003C/p>\n\u003C/blockquote>\n\u003Cp>I created a domain type appropriately called \u003Ccode>Document\u003C/code> that stores metadata about files on disk:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-go\">type Document struct {\n\tHash         string\n\tPath         string\n\tPreviousPath string\n\tOperation    *CognitiveOperation\n\tResults      *CognitiveReadResponse\n}\n\u003C/code>\u003C/pre>\n\u003Cp>I'll discuss the fields as they come up, but \u003Ccode>Path\u003C/code> and \u003Ccode>PreviousPath\u003C/code> should be obvious. Current and previous location on disk, so that I can account for file moves with at least a little bit of history.\u003C/p>\n\u003Cp>The pricing for the OCR is really attractive - as of September, 2019 it is:\u003C/p>\n\u003Cblockquote>\n\u003Cp>0-1M transactions — $1.50 per 1,000 transactions\u003C/p>\n\u003C/blockquote>\n\u003Cp>I know that I'll be fine tuning the processes that run, and likely running them repeatedly. I wanted to find a way to store the results from the OCR for each document, but I am also aware that I can't use the document name and path as the canonical key to find the document later, because the goal of this app is to move them and rename them appropriately! So I decided to use a hash of the file contents as a key. \u003Ccode>SHA256\u003C/code> seems to be the right algorithm for file contents, low cost computation, low collision chance. So I created a hash function that calculates the \u003Ccode>SHA256\u003C/code> hash of the document after it is read:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-go\">func (d *Document) GetHash() {\n\n\tf, err := os.Open(d.Path)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer f.Close()\n\n\th := sha256.New()\n\tif _, err := io.Copy(h, f); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\td.Hash = fmt.Sprintf(&quot;%x&quot;, h.Sum(nil))\n}\n\u003C/code>\u003C/pre>\n\u003Cp>After getting the results of the OCR operation, I set them in the \u003Ccode>Document\u003C/code> type, then persist the metadata to disk in a hidden directory. Currently that's \u003Ccode>~/.classifier/\u003C/code> but, as with all of this, it might change in the future.\u003C/p>\n\u003Cp>The file is stored using the \u003Ccode>SHA256\u003C/code> hash of the contents as the file name, and the \u003Ccode>Document\u003C/code> type is serialized to disk using Go's efficient and lightweight \u003Ccode>encoding/gob\u003C/code> format. While I'm debugging and playing with this code, I decided to also persist the data in \u003Ccode>json\u003C/code> format so it's easier to read. Here's the method on \u003Ccode>Document\u003C/code> that saves/serializes to disk:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-go\">func (d *Document) SaveMetadata() error {\n  fmt.Println(d.Hash)\n  //TODO use new XDG config dir location\n  // https://tip.golang.org/pkg/os/#UserConfigDir\n\tfilePath := &quot;/home/bjk/.classifier/&quot; + d.Hash // TODO FILEPATH.JOIN\n\tfmt.Println(filePath)\n\tfile, err := os.OpenFile(filePath, os.O_TRUNC|os.O_CREATE|os.O_WRONLY, 0644)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer file.Close()\n\tenc := gob.NewEncoder(file)\n\terr = enc.Encode(d)\n\tif err != nil {\n\t\treturn err\n\t}\n\tjfilePath := &quot;/home/bjk/.classifier/&quot; + d.Hash + &quot;.json&quot; // TODO FILEPATH.JOIN\n\tfmt.Println(jfilePath)\n\tjfile, err := os.OpenFile(jfilePath, os.O_TRUNC|os.O_CREATE|os.O_WRONLY, 0644)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer jfile.Close()\n\tjenc := json.NewEncoder(jfile)\n\treturn jenc.Encode(d)\n}\n\u003C/code>\u003C/pre>\n\u003Cp>Lots of bad things happening in there, see above caveats about copying/pasting this code. The important part is the encoding in \u003Ccode>gob\u003C/code> format of the contents of the \u003Ccode>Document\u003C/code> metadata, which is then saved to disk using the \u003Ccode>SHA256\u003C/code> hash as the filename. This is a nice future-proof solution, and provides several benefits.\u003C/p>\n\u003Cul>\n\u003Cli>If there is already a file with the same name, it's been processed once.\u003C/li>\n\u003Cli>If the \u003Ccode>.Path\u003C/code> is different from the document I'm inspecting, I might have an exact duplicate, which is a candidate for (soft) deleting\u003C/li>\n\u003Cli>It doesn't matter where the files get moved, as long as the \u003Ccode>SHA256\u003C/code> hash matches, I've got the metadata saved already.\u003C/li>\n\u003C/ul>\n\u003Cp>This is a very low-tech metadata database, of sorts. It's definitely not optimized for real-time use, but instead for batch operations.\u003C/p>\n\u003Cp>Keeping all the metadata in this format means I can write any number of other tools to read and modify the metadata without worrying too much.\u003C/p>\n\u003Ch3 id=\"step-two\">Step Two\u003C/h3>\n\u003Cp>At this point, I have a directory full of unprocessed files and a way to process them once and save the results so I don't have to re-process them later. It's time to fire off the processing app. I used \u003Ca href=\"https://github.com/spf13/cobra\">cobra\u003C/a> to build the command-line utility, so I made the root/naked command do the actual calls to Azure Cognitive Services:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-bash\">go build\n./classifier\n\u003C/code>\u003C/pre>\n\u003Cp>This iterates over every file in the \u003Ccode>~/Documents/Unfiled\u003C/code> directory, calling Cognitive Services OCR for the file types that are supported. There is no current mechanism to retrieve metadata from other document types (Word documents, text files, etc). That's a future addition.\u003C/p>\n\u003Cp>After receiving the results, the responses are serialized using the above mentioned \u003Ccode>gob\u003C/code> serialization into \u003Ccode>~/.classifier/HASH\u003C/code>\u003C/p>\n\u003Ch3 id=\"classification\">Classification\u003C/h3>\n\u003Cp>Based on the results there are some simple \u003Ccode>bag of words\u003C/code> matches that can be done. Some of the documents I have contain very unique text that is indicative of a particular document type. For example, Bank of America always includes my account number and their address in \u003Ccode>Wilmington\u003C/code>. No other document in my corpus has those two distinct things together, so I can write a simple classifier for all Bank of America documents. I decided to use simple TOML for a configuration file here:\u003C/p>\n\u003Cpre>\u003Ccode>[[entity]]\nname = &quot;Bank of America&quot;\ndirectory = &quot;BOA&quot;\nkeywords = [&quot;Bank of America&quot;,&quot;12345677889&quot;,&quot;Wilmington&quot;]\n\u003C/code>\u003C/pre>\n\u003Cp>Here, I added a sub-command in \u003Ccode>cobra\u003C/code> so I can classify files without re-posting them to Cognitive Services. So I added the \u003Ccode>classifier process\u003C/code> command:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-bash\">./classifier process\n\u003C/code>\u003C/pre>\n\u003Cp>It currently goes through all the files in \u003Ccode>Unfiled\u003C/code> and checks their metadata for matches against the TOML file. This worked perfectly for several of my external correspondents. It took all the documents from \u003Ccode>Unfiled\u003C/code> and placed them in \u003Ccode>Filed/{directory}\u003C/code>.\u003C/p>\n\u003Ch3 id=\"what-about-the-rest\">What About The Rest?\u003C/h3>\n\u003Cp>There are many documents that aren't easily processed this way though. My next inspiration came in the shower (of course). If you squint enough, or are far enough away, all documents from the same entity of the same type look the same. So all my mortgage statements look the same, but the numbers are different.\u003C/p>\n\u003Cp>I installed ImageMagick, and wrote a script to make a low-resolution thumbnail of each PDF. I made the resolution low enough that the text isn't readable even if you magnify the image.\u003C/p>\n\u003Cp>Then I searched for ways to compare images and came across \u003Ca href=\"https://github.com/rivo/duplo\">duplo\u003C/a>, which appears to do what I need. It does a hash of the image and allows you to compare other documents to that hash to find a similarity score. Using this type of process my next goal is to group similar documents together by searching for ones with matching or close-to-matching image hashes.\u003C/p>\n\u003Cp>But that'll be probably next weekend. It's been really fun doing this much, and I'm looking forward to seeing how much more I can learn as I go!\u003C/p>\n\u003Cp>Intermediate results:\u003C/p>\n\u003Cp>Before:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-bash\">2846 Files\n\u003C/code>\u003C/pre>\n\u003Cp>After:\u003C/p>\n\u003Cpre>\u003Ccode class=\"language-bash\">Unfiled\\\n  2710 Files\nFiled\\\n  136 Files in 2 Directories\n\u003C/code>\u003C/pre>\n\u003C!--kg-card-end: markdown-->","comment_id":"6282b1f8531f79020b2ba215","feature_image":"https://brian.dev/content/images/2022/05/IGa3Md8wP6g.jpg","featured":false,"visibility":"public","email_recipient_filter":"none","created_at":"2022-05-16T16:20:08.000-04:00","updated_at":"2022-05-16T16:22:43.000-04:00","published_at":"2019-09-03T16:20:00.000-04:00","custom_excerpt":"In this first post of (likely) a multi-part series I'm going to discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my `~/Documents/Unfiled` directory.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"62889299a640e2020be86e48","name":"AI/ML","slug":"ai-ml","description":"AI and Machine Learning","feature_image":"https://brian.dev/content/images/2022/05/bit_ai.png","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#2DA140","url":"https://brian.dev/tag/ai-ml/"},{"id":"62889299a640e2020be86e49","name":"Go","slug":"go","description":"The Go Programming Language","feature_image":"https://brian.dev/content/images/2022/05/GO_BUILD.png","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#40e6f2","url":"https://brian.dev/tag/go/"}],"authors":[{"id":"1","name":"Brian Ketelsen","slug":"brian","profile_image":"https://brian.dev/content/images/2022/05/logo-copy-1.png","cover_image":"https://brian.dev/content/images/2022/05/time-machine-1.jpg","bio":null,"website":null,"location":null,"facebook":null,"twitter":"@bketelsen","meta_title":null,"meta_description":null,"url":"https://brian.dev/author/brian/"}],"primary_author":{"id":"1","name":"Brian Ketelsen","slug":"brian","profile_image":"https://brian.dev/content/images/2022/05/logo-copy-1.png","cover_image":"https://brian.dev/content/images/2022/05/time-machine-1.jpg","bio":null,"website":null,"location":null,"facebook":null,"twitter":"@bketelsen","meta_title":null,"meta_description":null,"url":"https://brian.dev/author/brian/"},"primary_tag":{"id":"62889299a640e2020be86e48","name":"AI/ML","slug":"ai-ml","description":"AI and Machine Learning","feature_image":"https://brian.dev/content/images/2022/05/bit_ai.png","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":"#2DA140","url":"https://brian.dev/tag/ai-ml/"},"url":"https://brian.dev/organizing-documents-with-some-ai-ml-and-elbow-grease/","excerpt":"In this first post of (likely) a multi-part series I'm going to discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my `~/Documents/Unfiled` directory.","reading_time":6,"access":true,"send_email_when_published":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"email_subject":null,"frontmatter":null}}</script></div>
	</body>
</html>
